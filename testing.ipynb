{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "from nn import io\n",
    "import re\n",
    "from nn import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt file of sequences\n",
    "pos_seqs=io.read_text_file(\"./data/rap1-lieb-positives.txt\")\n",
    "neg_seqs=io.read_fasta_file(\"./data/yeast-upstream-1k-negative.fa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing functions\n",
    "\n",
    "# splitting imbalanced class labels\n",
    "def sample_seqs(seqs: List[str], labels: List[bool]) -> Tuple[List[str], List[bool]]:\n",
    "    \"\"\"\n",
    "    This function should sample the given sequences to account for class imbalance. \n",
    "    Consider this a sampling scheme with replacement.\n",
    "    \n",
    "    Args:\n",
    "        seqs: List[str]\n",
    "            List of all sequences.\n",
    "        labels: List[bool]\n",
    "            List of positive/negative labels\n",
    "\n",
    "    Returns:\n",
    "        sampled_seqs: List[str]\n",
    "            List of sampled sequences which reflect a balanced class size\n",
    "        sampled_labels: List[bool]\n",
    "            List of labels for the sampled sequences\n",
    "    \"\"\"\n",
    "    # we can use the positive classes as they are (all of them) and just sample the same number of negative classes from the large pool.\n",
    "    all_pos_seqs=[]\n",
    "    all_pos_labels=[]\n",
    "    all_neg_seqs=[]\n",
    "\n",
    "    # isolate positive and negativeclasses\n",
    "    for seq, label in zip(seqs, labels):\n",
    "        if (label==True):\n",
    "            all_pos_seqs.append(seq)\n",
    "            all_pos_labels.append(label)\n",
    "        else:\n",
    "            all_neg_seqs.append(seq)\n",
    "\n",
    "\n",
    "    # randomly sample negative classes in the same amount as there are positive classes\n",
    "    neg_seqs_sample=random.sample(all_neg_seqs, len(all_pos_seqs)) \n",
    "\n",
    "    sampled_seqs=all_pos_seqs+neg_seqs_sample\n",
    "    sampled_labels=all_pos_labels+[False]*len(neg_seqs_sample)\n",
    "\n",
    "    return sampled_seqs, sampled_labels\n",
    "    # pass\n",
    "\n",
    "# one-hot encode\n",
    "\"\"\"\n",
    "encodings for base pairs\n",
    "\n",
    "A -> [1, 0, 0, 0]\n",
    "T -> [0, 1, 0, 0]\n",
    "C -> [0, 0, 1, 0]\n",
    "G -> [0, 0, 0, 1]\n",
    "\"\"\"\n",
    "def one_hot_encode_seqs(seq_arr):\n",
    "    # define encodings # https://stackoverflow.com/questions/6116978/how-to-replace-multiple-substrings-of-a-string\n",
    "    encodings={'A':'1000', 'T':'0100', 'C':'0010', 'G':'0001'}\n",
    "    encodings=dict((re.escape(k), v) for k, v in encodings.items()) \n",
    "    pattern=re.compile(\"|\".join(encodings.keys()))\n",
    "\n",
    "    # encode over loop and store in new list; goal is to replace each base pair with a binary string then convert the entire binary string to a list to create the encodings\n",
    "    encoded_seq_arr=[]\n",
    "    for seq in seq_arr:\n",
    "        alt_seq=pattern.sub(lambda bp: encodings[re.escape(bp.group(0))], seq)\n",
    "        alt_seq=list(alt_seq)\n",
    "        encoded_seq_arr.append(alt_seq)\n",
    "\n",
    "    return encoded_seq_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture and nn initialization\n",
    "nn_arch=[{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'}, {'input_dim': 32, 'output_dim': 8, 'activation': 'sigmoid'}]\n",
    "global nn_arch\n",
    "\n",
    "# Seed NumPy\n",
    "seed=10\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define parameter dictionary\n",
    "param_dict = {}\n",
    "\n",
    "# Initialize each layer's weight matrices (W) and bias matrices (b)\n",
    "for idx, layer in enumerate(nn_arch):\n",
    "    layer_idx = idx + 1\n",
    "    input_dim = layer['input_dim']\n",
    "    output_dim = layer['output_dim']\n",
    "    param_dict['W' + str(layer_idx)] = np.random.randn(output_dim, input_dim) * 0.1\n",
    "    param_dict['b' + str(layer_idx)] = np.random.randn(output_dim, 1) * 0.1\n",
    "\n",
    "# returns param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def _sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def _relu(Z):\n",
    "    # https://www.digitalocean.com/community/tutorials/relu-function-in-python\n",
    "    return np.array([max(0.0, z) for z in Z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single forward pass private method\n",
    "def _single_forward(W_curr, b_curr, A_prev, activation=None):\n",
    "    Z_curr=np.dot(W_curr, A_prev)+b_curr\n",
    "\n",
    "    # pass through activation function\n",
    "    if (activation=='sigmoid'):\n",
    "        A_curr=_sigmoid(Z_curr)\n",
    "    elif (activation=='relu'):\n",
    "        A_curr=_relu(Z_curr)\n",
    "\n",
    "    return A_curr, Z_curr\n",
    "\n",
    "# define forward method for entire nn; will be similar to initialization but we use activation functions to derive the next A and Z matrices\n",
    "def forward(X):\n",
    "\n",
    "    # define cache as param_dict as we are going to update it\n",
    "    cache={}\n",
    "\n",
    "    # set A_curr as the input batches but transpose to have the batches along the columns and features along the rows; this matches it to the W matrix, which contains the weights for the current layer in each row\n",
    "    A_curr=X.T\n",
    "    cache['A0']=A_curr # add as initial layer\n",
    "\n",
    "    # go through each layer and call _single_forward\n",
    "    for idx, layer in enumerate(nn_arch):\n",
    "        A_prev=A_curr\n",
    "        layer_idx=idx+1\n",
    "        A_curr, Z_curr=_single_forward(param_dict['W' + str(layer_idx)], param_dict['b' + str(layer_idx)], A_prev, layer['activation'])\n",
    "\n",
    "        # add the A_curr and Z_curr to cache (on the last run, we get the final output layer)\n",
    "        cache['A' + str(layer_idx)]=A_curr # has dimensions output_dim x batch_size\n",
    "        cache['Z' + str(layer_idx)]=Z_curr # has dimensions output_dim x batch_size\n",
    "\n",
    "    return A_curr, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sigmoid_backprop(dA_curr, Z_curr):\n",
    "    pass\n",
    "\n",
    "def _relu_backprop(dA_curr, Z_curr):\n",
    "    pass\n",
    "\n",
    "def _binary_cross_entropy_backprop(y, y_hat):\n",
    "    pass\n",
    "\n",
    "def _mean_squared_error_backprop(y, y_hat):\n",
    "    pass\n",
    "\n",
    "\n",
    "# define arbitrary loss function\n",
    "_loss_func='bce' # or mse\n",
    "\n",
    "\n",
    "# backprop functions\n",
    "def _single_backprop(W_curr, b_curr, Z_curr, A_prev, dA_curr, activation_curr):\n",
    "    # dA_curr is the dC/dA_last, which is multiplied in the backpropagation step with dA/dZ, the output of this multiplication is dZ_curr, which represents dC/dZ -> we compute this first\n",
    "    # dA_curr has dimensions output_dim x batch_size, as does Z_curr and dZ_curr\n",
    "    if (activation_curr=='sigmoid'):\n",
    "        dZ_curr=_sigmoid_backprop(dA_curr, Z_curr)\n",
    "    elif (activation_curr=='relu'):\n",
    "        dZ_curr=_relu_backprop(dA_curr, Z_curr)\n",
    "\n",
    "    # dA_prev, represents dC/dA_prev, can be thought of as expanding the chain rule using dZcurr (dC/dZ) and dZ/dA_prev; the derivative of Z_curr with respect to A_prev is simply W_curr\n",
    "    # this means, taking the dot product of dZ_curr and W_curr will give us dA_prev\n",
    "    dA_prev=np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    # dZ/dW, the derivative of the linear combination with respect to weights, is simply A_prev, which has dimensions of previous layer, i.e. input_layer x batch_size\n",
    "    # W_curr, has dimensions output_dim x input_dim (A_prev length); so dW_curr must also be this dimension, and we can take the dot product of dZ_curr and A_prev.T\n",
    "    # Note, dW_curr represents dC/dW, which is the full chain multiplication of the subcomponents\n",
    "    dW_curr=np.dot(dZ_curr, A_prev.T)/dZ_curr.shape[1] # normalize by number of samples in batch (batch_size)\n",
    "\n",
    "    #\n",
    "    db_curr=None\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr\n",
    "\n",
    "def backprop(self, y: ArrayLike, y_hat: ArrayLike, cache: Dict[str, ArrayLike]):\n",
    "    # first compute dA_curr given the loss_function, y, and y_hat\n",
    "    if (_loss_func==\"bce\"):\n",
    "        dA_curr=_binary_cross_entropy_backprop(y, y_hat)\n",
    "    elif (_loss_func==\"mse\"):\n",
    "        dA_curr=_mean_squared_error_backprop(y, y_hat)\n",
    "\n",
    "\n",
    "    # initialize gradient dictionary\n",
    "    gradient_dict={}\n",
    "    \n",
    "    # the cache and param_dict is indexed such that the input layer is A0 and the final layer is AN where N is the number of layers. So with a three layer system, we would have A0, A1, and A2; The weights and biases are labeled for A1 and A2.\n",
    "    # iterate and enumerate the list in reverse to get the components that are fed into  \n",
    "    # https://stackoverflow.com/questions/529424/traverse-a-list-in-reverse-order-in-python\n",
    "    for idx, layer in reversed(list(enumerate(nn_arch))):\n",
    "        layer_idx=idx+1\n",
    "        dA_prev, dW_curr, db_curr=_single_backprop(param_dict['W'+str(layer_idx)], param_dict['b'+str(layer_idx)], cache['Z'+str(layer_idx)], cache['A'+str(idx)], dA_curr, layer['activation'])\n",
    "        dA_curr=dA_prev # update dA_curr as the previous dA since we are going backward\n",
    "\n",
    "        # update gradient dicts with the same labels as the param_dict\n",
    "        gradient_dict['W'+str(layer_idx)]=dW_curr\n",
    "        gradient_dict['b'+str(layer_idx)]=db_curr\n",
    "\n",
    "    return gradient_dict\n",
    "    # pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit function with minibatching\n",
    "batch_size=4 # set arbitrary batch size\n",
    "\n",
    "def fit(\n",
    "    self,\n",
    "    X_train: ArrayLike,\n",
    "    y_train: ArrayLike,\n",
    "    X_val: ArrayLike,\n",
    "    y_val: ArrayLike\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    This function trains the neural network by backpropagation for the number of epochs defined at\n",
    "    the initialization of this class instance.\n",
    "\n",
    "    Args:\n",
    "        X_train: ArrayLike\n",
    "            Input features of training set.\n",
    "        y_train: ArrayLike\n",
    "            Labels for training set.\n",
    "        X_val: ArrayLike\n",
    "            Input features of validation set.\n",
    "        y_val: ArrayLike\n",
    "            Labels for validation set.\n",
    "\n",
    "    Returns:\n",
    "        per_epoch_loss_train: List[float]\n",
    "            List of per epoch loss for training set.\n",
    "        per_epoch_loss_val: List[float]\n",
    "            List of per epoch loss for validation set.\n",
    "    \"\"\"\n",
    "    # intialize loss lists\n",
    "    per_epoch_loss_train=[]\n",
    "    per_epoch_loss_val=[]\n",
    "\n",
    "    # split training data into batches # https://www.geeksforgeeks.org/break-list-chunks-size-n-python/\n",
    "    X_train_batches=[X_train[i:i + batch_size] for i in range(0, len(X_train), batch_size)]  \n",
    "    y_train_batches=[y_train[i:i + batch_size] for i in range(0, len(y_train), batch_size)]  \n",
    "\n",
    "    # iterate and train wrt number of epoch\n",
    "    for e in range(self._epochs):\n",
    "        # all of the training is done with the batches iteratively and each time the loss is stored in the loss_train_list, which is averaged to give you the loss_train for the epoch\n",
    "        loss_train_list=[]\n",
    "        for X_train, y_train in zip(X_train_batches, y_train_batches):\n",
    "            # first train via forward alg. and get training loss\n",
    "            y_hat_train, cache_train=self.forward(X_train)\n",
    "            if (self._loss_func==\"bce\"): \n",
    "                loss_train=self._binary_cross_entropy_backprop(y_train, y_hat_train)\n",
    "            elif (self._loss_func==\"mse\"): \n",
    "                loss_train=self._mean_squared_error_backprop(y_train, y_hat_train)\n",
    "\n",
    "            loss_train_list.append(loss_train)\n",
    "\n",
    "            # update weights via backprop\n",
    "            grad_dict=self.backprop(y_train, y_hat_train, cache_train)\n",
    "            self._update_params(grad_dict)\n",
    "\n",
    "        # weighted average of the training losses where the weights are the length of each batch. If the batches are balanced in size, then the weighting does nothing and it is simply equal to the unweighted average\n",
    "        loss_train=(loss_train_list*[len(b) for b in X_train_batches])/len(X_train_batches)\n",
    "        per_epoch_loss_train.append(loss_train) # store training loss\n",
    "\n",
    "        # run validation on val data and store validation loss\n",
    "        y_hat_val, cache_val=self.forward(X_val)\n",
    "        if (self._loss_func==\"bce\"): \n",
    "            loss_val=self._binary_cross_entropy_backprop(y_val, y_hat_val)\n",
    "        elif (self._loss_func==\"mse\"): \n",
    "            loss_val=self._mean_squared_error_backprop(y_val, y_hat_val)\n",
    "        per_epoch_loss_val.append(loss_val) # store validation loss\n",
    "\n",
    "    return per_epoch_loss_train, per_epoch_loss_val\n",
    "    # pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/break-list-chunks-size-n-python/\n",
    "arr=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])  \n",
    "batch_size=6\n",
    "arr_split=[arr[i:i + batch_size] for i in range(0, len(arr), batch_size)]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arr_split_lens=[len(b) for b in arr_split]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 5]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_split_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_loss=np.array([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 10])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_loss*arr_split_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 sigmoid\n",
      "1 relu\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/529424/traverse-a-list-in-reverse-order-in-python\n",
    "for idx, layer in reversed(list(enumerate(nn_arch))):\n",
    "    layer_idx=idx+1\n",
    "    print(layer_idx, layer['activation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'},\n",
       " {'input_dim': 32, 'output_dim': 8, 'activation': 'sigmoid'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_arch[0]['input_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict['W1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict['b1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_t=np.array([[2,3], [3,4]])\n",
    "b_t=np.array([5,6])\n",
    "Z_t=np.array([7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch testing\n",
    "Z=np.array([[-1,1,-1,1,1,-1], [-1,1,-1,1,1,-1], [-1,1,-1,1,1,-1]])\n",
    "np.array([np.maximum(0,z) for z in Z.T])\n",
    "np.maximum(0,Z)\n",
    "np.where(Z>0, 1, 0)\n",
    "np.mean(Z, axis=1)\n",
    "\n",
    "\n",
    "np.array([np.maximum(0,z) for z in Z])\n",
    "\n",
    "z=np.array([-1,1,-1,1,1,-1])\n",
    "np.maximum(0,z)\n",
    "\n",
    "# loss_train_list=[2,3,4]\n",
    "# X_train_batches=[[4,3,2],[4,3,2],[4,3,2]]\n",
    "# (np.sum(np.array(loss_train_list)*np.array([len(b) for b in X_train_batches])))/len(9)\n",
    "\n",
    "# layer_idx=1\n",
    "# update=nn._lr*nn.grad_dict['b'+str(layer_idx)]\n",
    "\n",
    "# nn._param_dict['b'+str(layer_idx)]-=nn._lr*nn.grad_dict['b'+str(layer_idx)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
